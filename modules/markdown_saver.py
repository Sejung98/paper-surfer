"""
Markdown storage module
Save collected paper information in markdown format
"""

import os
import logging
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path
import json
import re

# Import config file
try:
    import config
except ImportError:
    print("Warning: config.py not found. Using default settings.")
    config = None

# Default configuration values
DEFAULT_MARKDOWN_TEMPLATE = """# {title}

## Paper Information
- **Title**: {title}
- **Authors**: {authors}
- **Journal**: {journal}
- **Publication Date**: {pub_date}
- **DOI**: {doi}
- **PMID**: {pmid}
- **PMC ID**: {pmc_id}
- **Keywords**: {keywords}

## Abstract
{abstract}

## Collection Information
- **Collection Date**: {collection_date}
- **Search Keywords**: {search_keyword}
- **Keyword Matching Score**: {keyword_score}
- **PubMed URL**: https://pubmed.ncbi.nlm.nih.gov/{pmid}/

## Metadata
- **Language**: {language}
- **Publication Type**: {publication_type}
- **MeSH Terms**: {mesh_terms}
- **Grant Information**: {grants}

---
*This document was automatically generated by Paper Surfer.*
"""

class MarkdownSaver:
    """Class to save paper information in Markdown format"""
    
    def __init__(self):
        # Load configuration
        output_dir = getattr(config, 'MARKDOWN_OUTPUT_DIR', './output/papers')
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.enable_date_folders = getattr(config, 'ENABLE_DATE_FOLDERS', True)
        self.file_prefix = getattr(config, 'MARKDOWN_FILE_PREFIX', 'paper')
        self.file_suffix = getattr(config, 'MARKDOWN_FILE_SUFFIX', '.md')
        self.template = getattr(config, 'MARKDOWN_TEMPLATE', DEFAULT_MARKDOWN_TEMPLATE)
        
        self.logger = logging.getLogger(__name__)
        
        # Create date-based subdirectories (depending on settings)
        if self.enable_date_folders:
            today = datetime.now().strftime('%Y-%m-%d')
            self.daily_dir = self.output_dir / today
            self.daily_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.daily_dir = self.output_dir
        
        # Store category information for filename prefix (no folder structure)
        self.category_folders = getattr(config, 'CATEGORY_FOLDERS', {
            "high": "high_relevance",
            "medium": "medium_relevance", 
            "low": "low_relevance"
        })
        
        # All papers will be saved directly in the daily directory (no subfolders)
        self.category_dirs = {}
        for category in self.category_folders.keys():
            self.category_dirs[category] = self.daily_dir
        
        self.logger.info(f"MarkdownSaver initialization complete: {self.daily_dir}")
        self.logger.info(f"All papers will be saved in single directory with category prefixes")
    
    def sanitize_filename(self, filename: str) -> str:
        """Remove characters that cannot be used in filenames"""
        # Remove characters that cannot be used in filenames
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '')
        
        # Replace consecutive spaces with single space
        filename = re.sub(r'\s+', ' ', filename)
        
        # Limit filename length (200 characters)
        if len(filename) > 200:
            filename = filename[:200]
        
        return filename.strip()
    
    def sanitize_journal_name(self, journal_name: str) -> str:
        """Sanitize and shorten journal name for filename"""
        if not journal_name:
            return ""
        
        # First sanitize the journal name
        sanitized = self.sanitize_filename(journal_name)
        
        # Common journal abbreviations
        abbreviations = {
            'Journal of': 'J',
            'journal of': 'J',
            'American Journal of': 'Am J',
            'International Journal of': 'Int J',
            'European Journal of': 'Eur J',
            'Proceedings of': 'Proc',
            'proceedings of': 'Proc',
            'Annals of': 'Ann',
            'annals of': 'Ann',
            'Nature': 'Nature',
            'Science': 'Science',
            'Cell': 'Cell',
            'Cancer': 'Cancer',
            'Oncology': 'Oncol',
            'Medicine': 'Med',
            'Research': 'Res',
            'Clinical': 'Clin',
            'Molecular': 'Mol',
            'Biological': 'Biol',
            'Experimental': 'Exp',
            'Genetics': 'Genet',
            'Genomics': 'Genomics'
        }
        
        # Apply abbreviations
        for full_form, abbrev in abbreviations.items():
            sanitized = sanitized.replace(full_form, abbrev)
        
        # Remove common words to shorten
        words_to_remove = ['the', 'and', 'of', 'for', 'in', 'on', 'at', 'to', 'with', 'by']
        words = sanitized.split()
        words = [word for word in words if word.lower() not in words_to_remove]
        
        # Limit to 50 characters and join
        result = ' '.join(words)
        if len(result) > 50:
            result = result[:50]
        
        return result.strip()
    
    def save_paper(self, paper_data: Dict) -> bool:
        """Save PubMed paper data as Markdown file"""
        try:
            # Generate filename
            title = paper_data.get('title', 'untitled_paper')
            pmid = paper_data.get('pmid', 'unknown_pmid')
            journal = paper_data.get('journal', 'Unknown_Journal')
            
            # Generate filename in format: [카테고리]_[저널이름][제목]
            sanitized_title = self.sanitize_filename(title)
            sanitized_journal = self.sanitize_journal_name(journal)
            
            # Get relevance category for filename prefix
            relevance_category = paper_data.get('relevance_category', 'medium')
            
            # Limit title length to prevent overly long filenames
            if len(sanitized_title) > 100:
                sanitized_title = sanitized_title[:100] + "..."
            
            if sanitized_journal:
                filename = f"{relevance_category}_{sanitized_journal}_{sanitized_title}"
            else:
                # Fallback if journal name is empty
                filename = f"{relevance_category}_Unknown_Journal_{sanitized_title}"
            
            # Add .md extension to filename
            if not filename.endswith('.md'):
                filename += '.md'
            
            # Determine category folder
            relevance_category = paper_data.get('relevance_category', 'medium')
            if relevance_category in self.category_dirs:
                target_dir = self.category_dirs[relevance_category]
            else:
                target_dir = self.daily_dir  # Default folder
            
            # Create file path
            filepath = target_dir / filename
            
            # Handle duplicate filenames
            counter = 1
            original_filepath = filepath
            while filepath.exists():
                stem = original_filepath.stem
                suffix = original_filepath.suffix
                filepath = target_dir / f"{stem}_{counter}{suffix}"
                counter += 1
            
            # Generate Markdown content
            markdown_content = self.template.format(**paper_data)
            
            # Save file
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            self.logger.info(f"Paper Markdown saved successfully: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"Markdown save failed: {e}")
            return False
    
    def create_paper_markdown(self, paper_info, 
                            keyword_analysis: Optional[Dict] = None, 
                            sections: Optional[Dict] = None) -> str:
        """Convert paper information to Markdown format (maintain backward compatibility)"""
        
        # Format keyword matching information
        keywords_text = ""
        if keyword_analysis and keyword_analysis.get('matches'):
            keywords_text = "### Keyword Matching Results\n"
            for keyword, info in keyword_analysis['matches'].items():
                if info['count'] > 0:
                    keywords_text += f"- **{keyword}**: {info['count']} matches\n"
            
            keywords_text += f"\n- **Total matches**: {keyword_analysis.get('total_matches', 0)}\n"
            keywords_text += f"- **Unique keywords found**: {keyword_analysis.get('unique_keywords_found', 0)}\n"
            keywords_text += f"- **Relevance score**: {keyword_analysis.get('score', 0):.2f}\n"
        
        # Format paper sections information
        sections_text = ""
        if sections:
            sections_text = "\n## Paper Sections\n"
            for section_name, section_content in sections.items():
                if section_content and section_content.strip():
                    sections_text += f"### {section_name.title()}\n"
                    sections_text += f"{section_content[:500]}{'...' if len(section_content) > 500 else ''}\n\n"
        
        # Use existing template (Google Scholar version)
        old_template = """
# {title}

## Paper Information
- **Title**: {title}
- **Authors**: {authors}
- **Publication Year**: {year}
- **Source**: {venue}
- **URL**: {url}

## Abstract
{abstract}

## Keyword Matching
{keywords}

## Collection Information
- **Collection Date**: {collection_date}
- **Matching Score**: {match_score}

---
"""
        
        markdown_content = old_template.format(
            title=paper_info.title or "No title",
            authors=paper_info.authors or "No author information",
            year=paper_info.year or "No year information",
            venue=paper_info.venue or "No source information",
            url=paper_info.url or "No URL",
            abstract=paper_info.abstract or "No abstract information",
            keywords=keywords_text,
            collection_date=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            match_score=keyword_analysis.get('score', 0) if keyword_analysis else 0
        )
        
        # Add sections information
        if sections_text:
            markdown_content += sections_text
        
        # Additional metadata
        markdown_content += f"\n## Additional Information\n"
        markdown_content += f"- **Citations**: {paper_info.citations}\n"
        if paper_info.pdf_url:
            markdown_content += f"- **PDF Link**: {paper_info.pdf_url}\n"
        if paper_info.scholar_url:
            markdown_content += f"- **Google Scholar**: {paper_info.scholar_url}\n"
        
        return markdown_content
    
    def save_paper_markdown(self, paper_info, 
                          keyword_analysis: Optional[Dict] = None, 
                          sections: Optional[Dict] = None,
                          custom_filename: Optional[str] = None) -> Optional[str]:
        """Save paper information as Markdown file (maintain backward compatibility)"""
        
        # Generate filename
        if custom_filename:
            filename = self.sanitize_filename(custom_filename)
        else:
            title = paper_info.title or "untitled_paper"
            filename = self.sanitize_filename(title)
        
        # Add prefix and suffix
        if self.file_prefix:
            filename = self.file_prefix + "_" + filename
        
        # Add .md extension to filename
        if not filename.endswith('.md'):
            filename += '.md'
        
        # Create file path
        filepath = self.daily_dir / filename
        
        # Handle duplicate filenames
        counter = 1
        original_filepath = filepath
        while filepath.exists():
            stem = original_filepath.stem
            suffix = original_filepath.suffix
            filepath = self.daily_dir / f"{stem}_{counter}{suffix}"
            counter += 1
        
        # Generate Markdown content
        markdown_content = self.create_paper_markdown(paper_info, keyword_analysis, sections)
        
        # Save file
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            self.logger.info(f"Paper Markdown saved successfully: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"Markdown save failed: {e}")
            return None
    
    def save_batch_papers(self, papers_data: List[Dict]) -> List[str]:
        """Batch save multiple papers"""
        saved_files = []
        
        for i, paper_data in enumerate(papers_data):
            paper_info = paper_data.get('paper_info')
            keyword_analysis = paper_data.get('keyword_analysis')
            sections = paper_data.get('sections')
            
            if not paper_info:
                self.logger.warning(f"No paper information: {i+1}")
                continue
            
            filepath = self.save_paper_markdown(paper_info, keyword_analysis, sections)
            if filepath:
                saved_files.append(filepath)
        
        self.logger.info(f"Total {len(saved_files)} paper Markdowns saved successfully")
        return saved_files
    
    def create_summary_markdown(self, papers_data: List[Dict], 
                              search_keywords: List[str]) -> Optional[str]:
        """Generate summary Markdown of collected papers"""
        
        summary_filename = f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        summary_filepath = self.daily_dir / summary_filename
        
        # Calculate summary statistics
        total_papers = len(papers_data)
        relevant_papers = sum(1 for p in papers_data if p.get('keyword_analysis', {}).get('score', 0) > 0)
        
        # Generate summary content
        summary_content = f"# Paper Collection Summary Report\n\n"
        summary_content += f"## Collection Information\n"
        summary_content += f"- **Collection Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        summary_content += f"- **Search Keywords**: {', '.join(search_keywords)}\n"
        summary_content += f"- **Total Papers**: {total_papers}\n"
        summary_content += f"- **Relevant Papers**: {relevant_papers}\n"
        summary_content += f"- **Relevance Ratio**: {(relevant_papers/total_papers*100):.1f}%\n\n"
        
        # Keyword statistics
        if papers_data:
            summary_content += f"## Keyword Matching Results\n"
            keyword_stats = {}
            for paper_data in papers_data:
                keyword_analysis = paper_data.get('keyword_analysis', {})
                if keyword_analysis.get('matches'):
                    for keyword, info in keyword_analysis['matches'].items():
                        if info['count'] > 0:
                            keyword_stats[keyword] = keyword_stats.get(keyword, 0) + info['count']
            
            for keyword, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True):
                summary_content += f"- **{keyword}**: {count} matches\n"
        
        # Paper list
        summary_content += f"\n## Collected Papers List\n"
        for i, paper_data in enumerate(papers_data, 1):
            paper_info = paper_data.get('paper_info')
            if paper_info:
                keyword_analysis = paper_data.get('keyword_analysis', {})
                score = keyword_analysis.get('score', 0)
                summary_content += f"{i}. **{paper_info.title}** (Relevance: {score:.2f})\n"
                summary_content += f"   - Authors: {paper_info.authors}\n"
                summary_content += f"   - Year: {paper_info.year}\n"
                summary_content += f"   - Source: {paper_info.venue}\n\n"
        
        # Save file
        try:
            with open(summary_filepath, 'w', encoding='utf-8') as f:
                f.write(summary_content)
            
            self.logger.info(f"Summary report saved successfully: {summary_filepath}")
            return str(summary_filepath)
            
        except Exception as e:
            self.logger.error(f"Summary report save failed: {e}")
            return None
    
    def save_metadata_json(self, papers_data: List[Dict]) -> Optional[str]:
        """Save metadata of collected papers as JSON"""
        
        metadata_filename = f"metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        metadata_filepath = self.daily_dir / metadata_filename
        
        # Construct metadata
        metadata = {
            'collection_date': datetime.now().isoformat(),
            'total_papers': len(papers_data),
            'papers': []
        }
        
        for paper_data in papers_data:
            paper_info = paper_data.get('paper_info')
            if paper_info:
                paper_metadata = {
                    'title': paper_info.title,
                    'authors': paper_info.authors,
                    'year': paper_info.year,
                    'venue': paper_info.venue,
                    'url': paper_info.url,
                    'abstract': paper_info.abstract,
                    'citations': paper_info.citations,
                    'pdf_url': paper_info.pdf_url,
                    'scholar_url': paper_info.scholar_url,
                    'keyword_analysis': paper_data.get('keyword_analysis', {}),
                    'sections': paper_data.get('sections', {}),
                    'is_relevant': paper_data.get('is_relevant', False)
                }
                metadata['papers'].append(paper_metadata)
        
        # Save JSON file
        try:
            with open(metadata_filepath, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Metadata JSON saved successfully: {metadata_filepath}")
            return str(metadata_filepath)
            
        except Exception as e:
            self.logger.error(f"Metadata JSON save failed: {e}")
            return None
    
    def get_saved_files_info(self) -> Dict:
        """Return information about saved files"""
        info = {
            'output_directory': str(self.daily_dir),
            'total_files': 0,
            'markdown_files': [],
            'other_files': []
        }
        
        try:
            for file_path in self.daily_dir.iterdir():
                if file_path.is_file():
                    info['total_files'] += 1
                    
                    file_info = {
                        'filename': file_path.name,
                        'size': file_path.stat().st_size,
                        'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
                    }
                    
                    if file_path.suffix.lower() == '.md':
                        info['markdown_files'].append(file_info)
                    else:
                        info['other_files'].append(file_info)
            
        except Exception as e:
            self.logger.error(f"File information collection failed: {e}")
        
        return info

# Usage example
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    # Test data
    test_paper_data = {
        'title': 'Test Paper',
        'authors': 'John Doe, Jane Smith',
        'journal': 'Test Journal',
        'pub_date': '2024-01-15',
        'doi': '10.1234/test.2024.001',
        'pmid': '12345678',
        'pmc_id': 'PMC1234567',
        'keywords': 'machine learning, AI',
        'abstract': 'This is the abstract of a test paper.',
        'collection_date': '2024-12-19 10:00:00',
        'search_keyword': 'machine learning',
        'keyword_score': '0.85',
        'language': 'eng',
        'publication_type': 'Journal Article',
        'mesh_terms': 'Machine Learning, Artificial Intelligence',
        'grants': 'NIH R01 GM123456'
    }
    
    # Test MarkdownSaver
    saver = MarkdownSaver()
    success = saver.save_paper(test_paper_data)
    print(f"Save successful: {success}") 